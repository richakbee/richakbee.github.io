<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  
  
  <meta name="generator" content="Wowchemy 5.0.0-beta.1 for Hugo">
  

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Richa Kaur">

  
  
  
    
  
  <meta name="description" content="Learn how to implemet a L layer Deep neural network from scratch in python .">

  
  <link rel="alternate" hreflang="en-us" href="https://richakbee.github.io/post/deep_neural_nets/">

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="https://richakbee.github.io/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/solarized-dark.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/solarized-dark.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js" integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    

  

  
  
  
    
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Dancing+Script:wght@500&display=swap">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="https://richakbee.github.io/css/wowchemy.52e1c6679ec042c253f501f7e7cad107.css">

  




  

  


  
  

  

  <link rel="manifest" href="https://richakbee.github.io/index.webmanifest">
  <link rel="icon" type="image/png" href="https://richakbee.github.io/images/icon_hu6bff33d6d21873592e5202e35658285b_19800_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="https://richakbee.github.io/images/icon_hu6bff33d6d21873592e5202e35658285b_19800_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://richakbee.github.io/post/deep_neural_nets/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@RK49708829">
  <meta property="twitter:creator" content="@RK49708829">
  
  <meta property="og:site_name" content="Richa Kaur">
  <meta property="og:url" content="https://richakbee.github.io/post/deep_neural_nets/">
  <meta property="og:title" content="Deep Neural Network from Scratch | Richa Kaur">
  <meta property="og:description" content="Learn how to implemet a L layer Deep neural network from scratch in python ."><meta property="og:image" content="https://richakbee.github.io/post/deep_neural_nets/featured.jpg">
  <meta property="twitter:image" content="https://richakbee.github.io/post/deep_neural_nets/featured.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-09-28T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-09-28T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://richakbee.github.io/post/deep_neural_nets/"
  },
  "headline": "Deep Neural Network from Scratch",
  
  "image": [
    "https://richakbee.github.io/post/deep_neural_nets/featured.jpg"
  ],
  
  "datePublished": "2020-09-28T00:00:00Z",
  "dateModified": "2020-09-28T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Richa Kaur"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Richa Kaur",
    "logo": {
      "@type": "ImageObject",
      "url": "https://richakbee.github.io/images/icon_hu6bff33d6d21873592e5202e35658285b_19800_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "Learn how to implemet a L layer Deep neural network from scratch in python ."
}
</script>

  

  


  


  





  <title>Deep Neural Network from Scratch | Richa Kaur</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper  ">

  
  
  
  
  
  
  
  
  <script src="https://richakbee.github.io/js/wowchemy-init.min.1df0392aab543464b23a85146803aaf2.js"></script>

  



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="https://richakbee.github.io/">Richa Kaur</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="https://richakbee.github.io/">Richa Kaur</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-center" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="https://richakbee.github.io/#about"><span>About Me</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="https://richakbee.github.io/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="https://richakbee.github.io/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="https://richakbee.github.io/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="https://richakbee.github.io/cv/"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <article class="article">

  




















  
  


<div class="article-container pt-3">
  <h1>Deep Neural Network from Scratch</h1>

  
  <p class="page-subtitle">Here we will understand the forward pass as well as the backward pass in a Deep neural network .</p>
  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Richa Kaur</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Sep 28, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    10 min read
  </span>
  

  
  
  
  
  
  <span class="middot-divider"></span>
  <a href="https://richakbee.github.io/post/deep_neural_nets/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="https://richakbee.github.io/category/deep-learning/">Deep Learning</a></span>
  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 322px; max-height: 156px;">
  <div style="position: relative">
    <img src="https://richakbee.github.io/post/deep_neural_nets/featured.jpg" alt="" class="featured-image">
    <span class="article-header-caption">Image credit: [<strong>sourced from google</strong>]</span>
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <h1 id="import-libraries">Import Libraries</h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
</code></pre>
<h2 id="architecture">Architecture</h2>
<p><img src="https://richakbee.github.io/images/DeepNNfromscratch/deepnn.jpg" alt="image.jpg"></p>
<h2 id="steps">steps</h2>
<blockquote>
<p>Forward Pass</p>
</blockquote>
<ul>
<li>initialize parameters (i.e weights &amp; biases)</li>
<li>caluclate linear forward ( a.k.a Z value)</li>
<li>calculate linear activation forward ( a.k.a A value)</li>
<li>calculate forward functions for all L layers</li>
<li>calculate Cost function</li>
</ul>
<blockquote>
<p>Backward Pass</p>
</blockquote>
<ul>
<li>calculate linear backward ( a.k.a derivatives)</li>
<li>calculate linear activation backward</li>
<li>calculate backward functions for all L layers</li>
<li>update the parameters (i.e weights &amp; biases)</li>
</ul>
<h2 id="forward-pass">Forward Pass</h2>
<h3 id="initialize-parameters">Initialize parameters</h3>
<ul>
<li>Random initialization to weights &amp; zeros to biases</li>
</ul>
<pre><code class="language-python">def initialize_parameters_deep(layers_dims):
    &quot;&quot;&quot;
    Arguments:
    layer_dims -- python array (list) containing the dimensions of each layer in our network
    
    Returns:
    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:
                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])
                    bl -- bias vector of shape (layer_dims[l], 1)
    &quot;&quot;&quot;
    
    np.random.seed(3)
    parameters={}
    L=len(layers_dims)
    
    for l in range(1,L):#loop goes from 1 to L-1
        parameters[&quot;W&quot;+str(l)]=np.random.randn(layers_dims[l],layers_dims[l-1])*0.01
        parameters[&quot;b&quot;+str(l)]=np.zeros((layers_dims[l],1))
    
        assert(parameters[&quot;W&quot;+str(l)].shape == (layers_dims[l], layers_dims[l-1]))
        assert(parameters[&quot;b&quot;+str(l)].shape == (layers_dims[l], 1))
    
    return parameters
</code></pre>
<pre><code class="language-python">layers_dims = np.array([3, 4,4, 1])
parameters = initialize_parameters_deep(layers_dims)
print(parameters)
</code></pre>
<pre><code>{'W1': array([[ 0.01788628,  0.0043651 ,  0.00096497],
       [-0.01863493, -0.00277388, -0.00354759],
       [-0.00082741, -0.00627001, -0.00043818],
       [-0.00477218, -0.01313865,  0.00884622]]), 'b1': array([[0.],
       [0.],
       [0.],
       [0.]]), 'W2': array([[ 0.00881318,  0.01709573,  0.00050034, -0.00404677],
       [-0.0054536 , -0.01546477,  0.00982367, -0.01101068],
       [-0.01185047, -0.0020565 ,  0.01486148,  0.00236716],
       [-0.01023785, -0.00712993,  0.00625245, -0.00160513]]), 'b2': array([[0.],
       [0.],
       [0.],
       [0.]]), 'W3': array([[-0.00768836, -0.00230031,  0.00745056,  0.01976111]]), 'b3': array([[0.]])}
</code></pre>
<h3 id="linear-forward">Linear Forward</h3>
<p>The linear forward module (vectorized over all the examples) computes the following equations:</p>
<p>$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\tag{4}$$
where $A^{[0]} = X$.</p>
<pre><code class="language-python">def linear_forward(A, W, b):
    &quot;&quot;&quot;
    Implement the linear part of a layer's forward propagation.

    Arguments:
    A -- activations from previous layer (or input data): (size of previous layer, number of examples)
    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
    b -- bias vector, numpy array of shape (size of the current layer, 1)

    Returns:
    Z -- the input of the activation function, also called pre-activation parameter 
    cache -- a python tuple containing &quot;A&quot;, &quot;W&quot; and &quot;b&quot; ; stored for computing the backward pass efficiently
    &quot;&quot;&quot;
 
    Z=np.dot(W,A)+b
    
    assert(Z.shape ==(W.shape[0],A.shape[1]))
    cache = (A,W,b)
    
    return Z,cache
</code></pre>
<h3 id="activation-functions">Activation functions</h3>
<pre><code class="language-python">def sigmoid(Z):
    Z=np.array(Z)
    return (1/(1+np.exp(-Z)),Z)
</code></pre>
<pre><code class="language-python">sigmoid([[1,2,3],[1,2,3]])
</code></pre>
<pre><code>(array([[0.73105858, 0.88079708, 0.95257413],
        [0.73105858, 0.88079708, 0.95257413]]), array([[1, 2, 3],
        [1, 2, 3]]))
</code></pre>
<pre><code class="language-python">def relu(Z):
    Z=np.array(Z)   
    return (np.where(Z&gt;0,Z,0),Z)
</code></pre>
<pre><code class="language-python">relu([[1,0.8,-3],[-1,2,0]])
</code></pre>
<pre><code>(array([[1. , 0.8, 0. ],
        [0. , 2. , 0. ]]), array([[ 1. ,  0.8, -3. ],
        [-1. ,  2. ,  0. ]]))
</code></pre>
<h3 id="linear-activation-forward">Linear activation forward</h3>
<p>Sigmoid: $\sigma(Z) = \sigma(W A + b) = \frac{1}{ 1 + e^{-(W A + b)}}$. We have provided you with the sigmoid function. This function returns two items: the activation value &ldquo;a&rdquo; and a &ldquo;cache&rdquo; that contains &ldquo;Z&rdquo; (it&rsquo;s what we will feed in to the corresponding backward function). To use it you could just call:</p>
<p>A, activation_cache = sigmoid(Z)
ReLU: The mathematical formula for ReLu is $A = RELU(Z) = max(0, Z)$. We have provided you with the relu function. This function returns two items: the activation value &ldquo;A&rdquo; and a &ldquo;cache&rdquo; that contains &ldquo;Z&rdquo; (it&rsquo;s what we will feed in to the corresponding backward function). To use it you could just call:</p>
<p>A, activation_cache = relu(Z)</p>
<pre><code class="language-python">def linear_activation_forward(A_prev, W, b, activation):
    &quot;&quot;&quot;
    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer

    Arguments:
    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)
    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
    b -- bias vector, numpy array of shape (size of the current layer, 1)
    activation -- the activation to be used in this layer, stored as a text string: &quot;sigmoid&quot; or &quot;relu&quot;

    Returns:
    A -- the output of the activation function, also called the post-activation value 
    cache -- a python tuple containing &quot;linear_cache&quot; and &quot;activation_cache&quot;;
             stored for computing the backward pass efficiently
    &quot;&quot;&quot;
    
    if activation=='sigmoid':
        Z,linear_cache = linear_forward(A_prev, W, b)
        A , activation_cache = sigmoid(Z) 
        
    if activation=='relu':
        Z,linear_cache = linear_forward(A_prev, W, b)
        A ,activation_cache = relu(Z)    
        
    assert (A.shape == (W.shape[0], A_prev.shape[1]))
    cache = (linear_cache, activation_cache)

    return A, cache    
</code></pre>
<h3 id="l-model-forward">L Model forward</h3>
<pre><code class="language-python">def L_model_forward(X, parameters):
    &quot;&quot;&quot;
    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation
    
    Arguments:
    X -- data, numpy array of shape (input size, number of examples)
    parameters -- output of initialize_parameters_deep()
    
    Returns:
    AL -- last post-activation value
    caches -- list of caches containing:
                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)
    &quot;&quot;&quot;
    AL=[]
    A=X
    caches=[]
    L=len(parameters)//2
    
    for l in range(1,L):
       
        A_prev =A
       
        A, cache = linear_activation_forward(A_prev,parameters[&quot;W&quot;+str(l)],parameters[&quot;b&quot;+str(l)],&quot;relu&quot;)
        caches.append(cache)
        
        
        
    AL , cache=linear_activation_forward(A,parameters[&quot;W&quot;+str(L)],parameters[&quot;b&quot;+str(L)],&quot;sigmoid&quot;)
    caches.append(cache)
    
    assert(AL.shape ==(1,X.shape[1]))
    return AL,caches
</code></pre>
<h3 id="cost-function">Cost Function</h3>
<pre><code class="language-python">def compute_cost(AL,Y):
    &quot;&quot;&quot;
    Implement the cost function defined by equation (7).

    Arguments:
    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)
    Y -- true &quot;label&quot; vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)

    Returns:
    cost -- cross-entropy cost
    &quot;&quot;&quot;
    m = Y.shape[1]
    
    cost = -1/m*np.sum(Y*np.log(AL)+(1-Y)*np.log((1-AL)))

    
    cost = np.squeeze(cost) # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).
    assert(cost.shape == ())
    
    return cost
</code></pre>
<h2 id="backward-pass">Backward Pass</h2>
<h3 id="linear-backward">Linear Backward</h3>
<p>For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).</p>
<p>Suppose you have already calculated the derivative $dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.</p>
<p><img src="https://richakbee.github.io/images/DeepNNfromscratch/linback.jpg" alt="image.png"></p>
<p>The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$ (using formulae above).</p>
<!-- Here are the formulas you need:
$$ dW^{[l]} = \frac{\partial \mathcal{J} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T} \tag{8}$$ $$ db^{[l]} = \frac{\partial \mathcal{J} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)}tag{9}$$
$$ dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \tag{10}$$ -->
<pre><code class="language-python">def linear_backward(dZ,cache):
    &quot;&quot;&quot;
    Implement the linear portion of backward propagation for a single layer (layer l)

    Arguments:
    dZ -- Gradient of the cost with respect to the linear output (of current layer l)
    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

    Returns:
    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW -- Gradient of the cost with respect to W (current layer l), same shape as W
    db -- Gradient of the cost with respect to b (current layer l), same shape as b
    &quot;&quot;&quot;
    A_prev, W, b = cache
    m = A_prev.shape[1]
    
    dW = 1/m*np.dot(dZ,A_prev.T)
    db =1/m*np.sum(dZ,axis=1,keepdims=True)
    dA_prev =np.dot(W.T,dZ)
    
    assert(dA_prev.shape == A_prev.shape)
    assert(dW.shape == W.shape)
    assert(db.shape == b.shape)
    
    return dA_prev, dW, db
</code></pre>
<h3 id="backward-activation-functions">Backward activation functions</h3>
<pre><code class="language-python">def sigmoid_backward(dA, activation_cache):
    &quot;&quot;&quot;
    activation_cache= Z
    &quot;&quot;&quot;
    Z=np.array(activation_cache) 
    val=1/(1+np.exp(-Z))
    return (dA*val*(1-val))
</code></pre>
<pre><code class="language-python">sigmoid_backward([[1,2,3],[1,2,3]],[[1,2,3],[1,2,3]])
</code></pre>
<pre><code>array([[0.19661193, 0.20998717, 0.13552998],
       [0.19661193, 0.20998717, 0.13552998]])
</code></pre>
<pre><code class="language-python">def relu_backward(dA, activation_cache):
    &quot;&quot;&quot;
    activation_cache= Z
    &quot;&quot;&quot;
    Z=np.array(activation_cache)
    return (dA*np.where(Z&gt;0,1,0))
</code></pre>
<pre><code class="language-python">relu_backward([[1,2,3],[1,2,3]],[[1,0.8,-3],[-1,2,0]])
</code></pre>
<pre><code>array([[1, 2, 0],
       [0, 2, 0]])
</code></pre>
<h3 id="linear-activation-backward">Linear activation backward</h3>
<p>To help  implement linear_activation_backward, we provided two backward functions:</p>
<p>sigmoid_backward: Implements the backward propagation for SIGMOID unit. You can call it as follows:
dZ = sigmoid_backward(dA, activation_cache)
relu_backward: Implements the backward propagation for RELU unit. You can call it as follows:
dZ = relu_backward(dA, activation_cache)
If $g(.)$ is the activation function, sigmoid_backward and relu_backward compute$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \tag{11}$$</p>
<pre><code class="language-python">def linear_activation_backward(dA, cache, activation):
    &quot;&quot;&quot;
    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.
    
    Arguments:
    dA -- post-activation gradient for current layer l 
    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently
    activation -- the activation to be used in this layer, stored as a text string: &quot;sigmoid&quot; or &quot;relu&quot;
    
    Returns:
    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW -- Gradient of the cost with respect to W (current layer l), same shape as W
    db -- Gradient of the cost with respect to b (current layer l), same shape as b
    &quot;&quot;&quot;
    linear_cache, activation_cache = cache
    
    if activation == &quot;sigmoid&quot;: 
        dZ =sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)
      
    elif activation == &quot;relu&quot;:
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)
    
    return dA_prev, dW, db
</code></pre>
<h3 id="l-model-backward">L model backward</h3>
<pre><code class="language-python">def L_model_backward(AL, Y, caches):
    &quot;&quot;&quot;
    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group
    
    Arguments:
    AL -- probability vector, output of the forward propagation (L_model_forward())
    Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat)
    caches -- list of caches containing:
                every cache of linear_activation_forward() with &quot;relu&quot; (it's caches[l], for l in range(L-1) i.e l = 0...L-2)
                the cache of linear_activation_forward() with &quot;sigmoid&quot; (it's caches[L-1])
    
    Returns:
    grads -- A dictionary with the gradients
             grads[&quot;dA&quot; + str(l)] = ... 
             grads[&quot;dW&quot; + str(l)] = ...
             grads[&quot;db&quot; + str(l)] = ... 
    &quot;&quot;&quot;
    grads = {}
    L = len(caches) # the number of layers
    m = AL.shape[1]
    Y = Y.reshape(AL.shape)# after this line, Y is the same shape as AL
    
    # Initializing the backpropagation
    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
    
    ###Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: &quot;dAL, current_cache&quot;. 
    ###Outputs: &quot;grads[&quot;dAL-1&quot;], grads[&quot;dWL&quot;], grads[&quot;dbL&quot;]
    current_cache = caches[L-1]
    grads[&quot;dA&quot; + str(L-1)], grads[&quot;dW&quot; + str(L)], grads[&quot;db&quot; + str(L)] =linear_activation_backward(dAL, current_cache, &quot;sigmoid&quot;)
    
    # Loop from l=L-2 to l=0
    for l in reversed(range(L-1)):
        # lth layer: (RELU -&gt; LINEAR) gradients.
        # Inputs: &quot;grads[&quot;dA&quot; + str(l + 1)], current_cache&quot;. 
        ###Outputs: &quot;grads[&quot;dA&quot; + str(l)] , grads[&quot;dW&quot; + str(l + 1)] , grads[&quot;db&quot; + str(l + 1)] 
        ### START CODE HERE ### (approx. 5 lines)
        current_cache =caches[l]
        dA_prev_temp, dW_temp, db_temp =linear_activation_backward(grads[&quot;dA&quot; + str(l+1)], current_cache, activation = &quot;relu&quot;)
        grads[&quot;dA&quot; + str(l)] = dA_prev_temp
        grads[&quot;dW&quot; + str(l + 1)] = dW_temp
        grads[&quot;db&quot; + str(l + 1)] =db_temp 
        
    return grads  
</code></pre>
<h3 id="update-parameters">Update Parameters</h3>
<p>update the parameters of the model, using gradient descent:</p>
<p>$$ W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} $$$$ b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} $$
where $\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary.</p>
<p>Implement update_parameters() to update your parameters using gradient descent.</p>
<pre><code class="language-python">def update_parameters(parameters, grads, learning_rate):  
    
    L = len(parameters) // 2 # number of layers in the neural network
    
    # Update rule for each parameter.
   
    for l in range(L):
        parameters[&quot;W&quot; + str(l+1)] = parameters[&quot;W&quot; + str(l+1)]-learning_rate*grads[&quot;dW&quot; + str(l+1)]
        parameters[&quot;b&quot; + str(l+1)] =  parameters[&quot;b&quot; + str(l+1)]-learning_rate*grads[&quot;db&quot; + str(l+1)]
        
  
  
    return parameters
</code></pre>
<h1 id="l-layer-neural-networkfinal">L-layer neural network(Final)</h1>
<p>Question: Use the helper functions you have implemented in the previous assignment to build a 2-layer neural network with the following structure: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. The functions you may need and their inputs are:</p>
<p><img src="https://richakbee.github.io/images/DeepNNfromscratch/funcsteps.jpg" alt="image.png"></p>
<pre><code class="language-python">def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):
    &quot;&quot;&quot;
    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.
    
    Arguments:
    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)
    Y -- true &quot;label&quot; vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)
    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).
    learning_rate -- learning rate of the gradient descent update rule
    num_iterations -- number of iterations of the optimization loop
    print_cost -- if True, it prints the cost every 100 steps
    
    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    &quot;&quot;&quot;

    np.random.seed(3)
    costs = []                             # to keep track of the cost
    
    
    # Initialize parameters dictionary
    parameters = initialize_parameters_deep(layers_dims)
        
   # Loop (gradient descent)

    for i in range(0, num_iterations):
         # Forward propagation: 
        #[LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.
        AL, caches =  L_model_forward(X, parameters)

        
        # Compute cost
        cost = compute_cost(AL, Y)

        # Backward propagation.
        grads = L_model_backward(AL, Y, caches)
        
         # Update parameters.
        
        ### START CODE HERE ### (approx. 1 line of code)
        parameters = update_parameters(parameters, grads, learning_rate)
           
        if print_cost and i % 100 == 0:
            print(&quot;Cost after iteration {}: {}&quot;.format(i, np.squeeze(cost)))
        if print_cost and i % 100 == 0:
            costs.append(cost)
    
    plt.plot(np.squeeze(costs))
    plt.ylabel('cost')
    plt.xlabel('iterations (per hundreds)')
    plt.title(&quot;Learning rate =&quot; + str(learning_rate))
    plt.show()
    
    return parameters
</code></pre>
<h1 id="implement-4-layer-neural-network">Implement 4-layer neural network</h1>
<pre><code class="language-python">layers_dims = ([3,4,5,2,1])
</code></pre>
<pre><code class="language-python">X=np.random.randn(3,150)
Y=[]
for i in range(150):
    Y.append(np.random.randint(0,2))
Y=np.array(Y)    
Y=np.reshape(Y,(1,150))
print(X.shape,Y.shape)
</code></pre>
<pre><code>(3, 150) (1, 150)
</code></pre>
<pre><code class="language-python">L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=True)
</code></pre>
<pre><code>Cost after iteration 0: 0.693147179101668
Cost after iteration 100: 0.6913663615472958
Cost after iteration 200: 0.6901428329273512
Cost after iteration 300: 0.6893019578750604
Cost after iteration 400: 0.6887238512106988
.
.
.
.
.
.
Cost after iteration 2700: 0.6874476979055951
Cost after iteration 2800: 0.6874476213206875
Cost after iteration 2900: 0.6874475684872733
</code></pre>
<p><img src="https://richakbee.github.io/images/DeepNNfromscratch/output_40_1.png" alt="png"></p>
<pre><code> #looking at the parameters learnt
{'W1': array([[ 0.01788646,  0.00436687,  0.00096225],
        [-0.01863628, -0.00277482, -0.00354674],
        [-0.00082677, -0.00627067, -0.00043808],
        [-0.00476898, -0.01314048,  0.00884857]]),
 'b1': array([[-2.64082146e-07],
        [ 2.72834707e-07],
        [ 3.64257323e-07],
        [ 1.75856439e-06]]),
 'W2': array([[ 0.0088149 ,  0.01709711,  0.00049992, -0.00404753],
        [-0.0054536 , -0.01546477,  0.00982367, -0.01101068],
        [-0.01185051, -0.00205634,  0.01486141,  0.00236701],
        [-0.01023786, -0.00712985,  0.00625262, -0.00160502],
        [-0.00768738, -0.00230125,  0.00745106,  0.01976248]]),
 'b2': array([[ 2.34080642e-05],
        [ 0.00000000e+00],
        [-2.81738458e-06],
        [ 1.85347160e-05],
        [ 8.13445992e-05]]),
 'W3': array([[-0.01244123, -0.00626417, -0.00803766, -0.02419083, -0.00923792],
        [-0.01024285,  0.01123978, -0.00131827, -0.01623289,  0.0064712 ]]),
 'b3': array([[0.        ],
        [0.00374547]]),
 'W4': array([[-0.00356271, -0.01783282]]),
 'b4': array([[-0.21327127]])}
</code></pre>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="https://richakbee.github.io/tag/jupyter/">jupyter</a>
  
  <a class="badge badge-light" href="https://richakbee.github.io/tag/deep-learning/">Deep Learning</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://richakbee.github.io/post/deep_neural_nets/&amp;text=Deep%20Neural%20Network%20from%20Scratch" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://richakbee.github.io/post/deep_neural_nets/&amp;t=Deep%20Neural%20Network%20from%20Scratch" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Deep%20Neural%20Network%20from%20Scratch&amp;body=https://richakbee.github.io/post/deep_neural_nets/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://richakbee.github.io/post/deep_neural_nets/&amp;title=Deep%20Neural%20Network%20from%20Scratch" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Deep%20Neural%20Network%20from%20Scratch%20https://richakbee.github.io/post/deep_neural_nets/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://richakbee.github.io/post/deep_neural_nets/&amp;title=Deep%20Neural%20Network%20from%20Scratch" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://richakbee.github.io/"><img class="avatar mr-3 avatar-circle" src="https://richakbee.github.io/authors/admin/avatar_hua455b422af7d975d90da9c069dac9515_482055_270x270_fill_lanczos_center_2.png" alt="Richa Kaur"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://richakbee.github.io/">Richa Kaur</a></h5>
      <h6 class="card-subtitle">Business Analyst</h6>
      <p class="card-text">Business Analyst by Day , Machine Learning hobbyist by night</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:rich.kaur1718@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/RK49708829" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/richakbee" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/richa-kaur-931500141/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  







<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  var disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "richakbee.discuss.com" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>




<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="https://richakbee.github.io/post/cnn_from_scratch/" rel="next">Convolution Neural Network from Scratch</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="https://richakbee.github.io/post/nlp-demo/" rel="prev">Resturant reviews with nltk</a>
  </div>
  
</div>

</div>





  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="https://richakbee.github.io/post/cnn_from_scratch/">Convolution Neural Network from Scratch</a></li>
      
      <li><a href="https://richakbee.github.io/post/nlp-demo/">Resturant reviews with nltk</a></li>
      
    </ul>
  </div>
  





  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">
  

  <p class="powered-by">
    ©.2022
  </p>

  
  






  <p class="powered-by">
    
    
    
    Published with
    <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a>  —
    the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">
    open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.0/mermaid.min.js" integrity="sha512-ja+hSBi4JDtjSqc4LTBsSwuBT3tdZ3oKYKd07lTVYmCnTCor56AnRql00ssqnTOR9Ss4gOP/ROGB3SfcJnZkeg==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    

    
    

    

    
    

    
    

    
    
    

    
    

    
    
    <script id="dsq-count-scr" src="https://richakbee.discuss.com.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
    
    
    <script src="https://richakbee.github.io/js/wowchemy.min.023d21ed9274c9619f3bcab353d9a10c.js"></script>

    






</body>
</html>
